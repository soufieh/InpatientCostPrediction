# -*- coding: utf-8 -*-
"""InpatientCostPrediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gyBqQEnZYu4BJtFIjTa-EV-rvdlSF-oa
"""

from google.colab import drive
drive.mount('/content/drive')

from sklearn import datasets
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
from sklearn import tree
from sklearn import preprocessing
from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor

df0 = pd.read_csv('/content/drive/MyDrive/Inpatient_Data.csv')
print(df0.head())

df0.columns

"""**Finding Initial Correlation**"""

f, ax = plt.subplots(figsize=(11, 9))
corr = df0.corr()
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
);

"""**Pre-Processing**"""

# Checking for redundancy and removing those that are redundant
print(len(df0['APR DRG Code'].unique()))
print(len(df0['APR DRG Description'].unique()))

print(len(df0['APR Severity of Illness Code'].unique()))
print(len(df0['APR Severity of Illness Description'].unique()))

print(len(df0['APR MDC Description'].unique()))
print(len(df0['APR MDC Code'].unique()))

print(len(df0['CCS Diagnosis Description'].unique()))
print(len(df0['CCS Diagnosis Code'].unique()))

print(len(df0['CCS Procedure Description'].unique()))
print(len(df0['CCS Procedure Code'].unique()))

print( df0['Payment Typology 3'])

#Dropping Zipcode, Hospital County, Hospital Service Area as the facility name will grant us all that same info
df1 = df0.drop(['Ratio of Total Costs to Total Charges','Hospital County','Hospital Service Area','Zip Code - 3 digits', 'Payment Typology 3','APR DRG Code','APR MDC Code','APR Severity of Illness Code', 'Discharge Year','Operating Certificate Number','Permanent Facility Id','CCS Diagnosis Code','CCS Procedure Code'], axis=1); #Dropping Payment Typology 3, the information is negligible in our cost transparency estimation and we want to retain as much of the dataset as possible
df1.isna().sum()

df1.shape

df2 = df1.dropna()
df2.count()

print(len(df2.columns))

df2.isna().sum()

df2.replace({'120 +': 120},regex=True,inplace=True)
df2['Total Charges']=df2['Total Charges'].astype(str).str.replace(',','')
df2['Total Costs']=df2['Total Costs'].astype(str).str.replace(',','')
df2['Length of Stay'] = df2['Length of Stay'].astype(int)
df2['Total Charges'] = df2['Total Charges'].astype(float)
df2['Total Costs'] = df2['Total Costs'].astype(float)
df2.info()

df2.loc[:,"APR Severity of Illness Description"].value_counts()

#replace illness severity with ordinal numbers 
df2["APR Severity of Illness Description"].replace(["Moderate","Minor","Major","Extreme"], [2,1,3,4],)

#replace APR risk severity with ordinal numbers 
df2["APR Risk of Mortality"].replace(["Moderate","Minor","Major","Extreme"], [2,1,3,4])

#see the first few rows in the dataset
df2.head()

#clean all the column names 
col_names = list(df2.columns)
new_col_names = [col.replace(" ", "_") for col in col_names]
df2.columns = new_col_names

df2.head()

df2.Facility_Name.value_counts()

df2.Ethnicity.value_counts()

df2.Type_of_Admission.value_counts()

df2.Age_Group.value_counts()

df2.Age_Group.replace(to_replace=["0 to 17","18 to 29","30 to 49","50 to 69","70 or Older"], value=[1,2,3,4,5], inplace=True)

df2.Patient_Disposition.value_counts()

df2.APR_Medical_Surgical_Description.value_counts()

df2.APR_Severity_of_Illness_Description.value_counts()

df2.APR_Severity_of_Illness_Description.replace(to_replace=["Minor","Moderate","Major","Extreme"], value=[1,2,3,4], inplace=True)

df2.APR_Risk_of_Mortality.value_counts()

df2.APR_Risk_of_Mortality.replace(to_replace=["Minor","Moderate","Major","Extreme"], value=[1,2,3,4], inplace=True)
#print(df2['APR_Risk_of_Mortality'])

df2.APR_Medical_Surgical_Description.value_counts()

df2.Payment_Typology_1.value_counts()

df2.Payment_Typology_2.value_counts()

df2.Emergency_Department_Indicator.value_counts()

df2.Abortion_Edit_Indicator.value_counts()

df2.head()

df2.shape

X1 = df2.drop(labels=['Total_Costs'], axis=1) #Dropping 'Total_costs' from X dataset 
y1 = df2.loc[:,"Total_Costs"]

#create the training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.1)
print('done')

X_train.shape

X_test.shape

y_train.shape

y_test.shape

#seperate categorical from numerical columns
df_cols = list(X_train.columns)

cat_cols = [ cols for cols in df_cols if X_train.loc[:,cols].dtype=="object"]

num_cols = list(set(X_train.columns) - set(cat_cols))

cat_cols

num_cols

X_train.head()

feature_scaling = StandardScaler()
encoder = OneHotEncoder(handle_unknown="ignore")

#this columns transformer encodes catgeorical features and standardize numerical features
column_tf = make_column_transformer(
                                    (encoder, cat_cols),
                                    (feature_scaling, num_cols),
                                    remainder ="passthrough")

X_train_preprocessed = column_tf.fit_transform(X_train)
X_test_preprocessed = column_tf.transform(X_test)

print(X_train_preprocessed.shape)
#print(len(column_tf.get_feature_names_out()))
columns=column_tf.get_feature_names_out()
print(len(columns))

"""**Linear Regression**"""

#using linear regression
clf = LinearRegression(n_jobs=-1)
clf.fit(X_train_preprocessed,y_train)
y_pred=clf.predict(X_test_preprocessed)
print('done')

#print("RMSE:",metrics.max_error(y_test, y_pred))
print("R2 value:",metrics.r2_score(y_test, y_pred))
#print("Adj R2 value:",metrics.r2_score(y_test, y_pred))
print("MSE:",metrics.mean_squared_error(y_test, y_pred))

r_sq = metrics.r2_score(y_test, y_pred)
k = X_test.shape[1]
n = X_test.shape[0]
adj_R2 = 1 - (((1-r_sq)*(n-1))/(n-k-1))
adj_R2

plt.figure(figsize=(10,10))
plt.scatter(y_test, y_pred, c='crimson')

p1 = max(max(y_pred), max(y_test))
p2 = min(min(y_pred), min(y_test))
plt.plot([p1, p2], [p1, p2], 'b-')
plt.xlabel('True Values', fontsize=15)
plt.ylabel('Predictions', fontsize=15)
plt.axis('equal')
plt.show()

"""**Decision Tree**"""

dtr= DecisionTreeRegressor(random_state=0)

dtr.fit(X_train_preprocessed,y_train)
print('done')

y_pred=dtr.predict(X_test_preprocessed)
print('done')

#print("RMSE:",metrics.max_error(y_test, y_pred))
print("R2 value:",metrics.r2_score(y_test, y_pred))
#print("Adj R2 value:",metrics.r2_score(y_test, y_pred))
print("MSE:",metrics.mean_squared_error(y_test, y_pred))

r_sq = metrics.r2_score(y_test, y_pred)
k = X_test.shape[1]
n = X_test.shape[0]
adj_R2 = 1 - (((1-r_sq)*(n-1))/(n-k-1))
adj_R2

plt.figure(figsize=(10,10))
plt.scatter(y_test, y_pred, c='crimson')

p1 = max(max(y_pred), max(y_test))
p2 = min(min(y_pred), min(y_test))
plt.plot([p1, p2], [p1, p2], 'b-')
plt.xlabel('True Values', fontsize=15)
plt.ylabel('Predictions', fontsize=15)
plt.axis('equal')
plt.xlim([-.25, 2])
plt.show()

"""**Random Forest Regressor** --> Could Not Run"""

#fit a random forest regressor
rfr=RandomForestRegressor(n_estimators=100, verbose = 1,random_state=0)
rfr.fit(X_train_preprocessed,y_train)
y_pred=rfr.predict(X_test_preprocessed)
print('done')

print("Accuracy:",metrics.max_error(y_test, y_pred))
print("Accuracy:",metrics.r2_score(y_test, y_pred))
print("Accuracy:",metrics.mean_squared_error(y_test, y_pred))

"""**Support Vector Regression** --> Could Not Run"""

sreg= SVR(kernel = 'linear', verbose = True)

sreg.fit(X_train_preprocessed,y_train)
y_pred= sreg.predict(X_test_preprocessed)
print('done')

"""**Apply PCA** --> Could not run"""

from sklearn.decomposition import PCA

X_train_preprocessed.todense()

pca = PCA(random_state=0)
pca.fit(X_train_preprocessed.todense())

"""**Apply Lasso**"""

reg = Lasso()
reg.fit(X_train_preprocessed, y_train)
print("Best alpha using built-in Lasso: %f" % reg.alpha)
print("Best score using built-in Lasso: %f" %reg.score(X_test_preprocessed,y_test))

coef = pd.Series(reg.coef_)
print(X_train_preprocessed.shape)
print(len(coef))

print(X_train_preprocessed.shape)
print(len(coef))

print("Lasso picked " + str(sum(coef != 0)) + " variables and eliminated the other " +  str(sum(coef == 0)) + " variables")

Feature_name = []
Feature_coef = []

for i in range(len(coef)):
  if coef[i] != 0: 
    Feature_name.append(columns[i])
    Feature_coef.append(coef[i])

true_label = pd.DataFrame({'Feature Name': Feature_name, 'Feature Coef': Feature_coef})

True_Labels = true_label.sort_values(by = ['Feature Coef'], ascending=False)
print(True_Labels.head(15))

import matplotlib
matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)
True_Labels.iloc[0:16].plot(kind = "barh")
plt.title("Feature importance using Lasso Model")